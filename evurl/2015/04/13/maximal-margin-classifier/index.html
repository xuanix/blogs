<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Write your site description here. It will be used as your sites meta description as well!">
    <meta name="author" content="Tianxiang Gao">

    <title>Suport vector machine #2 - Maximal margin classifier - Tianxiang's Blog</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/blogs/css/bootstrap.css">
    <link rel="stylesheet" href="/blogs/css/style.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/blogs/css/clean-blog.css">
    <link rel="stylesheet" href="/blogs/css/main.css">

    <!-- Pygments Github CSS 
    <link rel="stylesheet" href="/blogs/css/syntax.css"> -->

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>


<body>

    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                <i class="fa fa-bars"></i>
            </button>
            <a class="navbar-brand" href="http://gaotx.com//blogs">
                <h1>Tianxiang Gao</h1>
            </a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="http://gaotx.com/#intro">Home</a></li>
            <li><a href="http://gaotx.com/#about">About</a></li>
            <li><a href="http://gaotx.com/#portfolio">News</a></li>
            <li><a href="http://gaotx.com/#contact">Contact</a></li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown">Blogs <b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><a href="http://gaotx.com/blogs/archive">Archive</a></li>
                <li><a href="#">Categories</a></li>
                <li><a href="#">Tags</a></li>
              </ul>
            </li>
          </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Post Header -->
<header class="intro-header" style="background-image: url('/blogs/img/post-bg-02.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Suport vector machine #2 - Maximal margin classifier</h1>
                    
                    <h2 class="subheading">Suport vector machine is an extention of a simple and intuitive classifier called the maximal margin classifier</h2>
                    
                    <span class="meta">Posted by Tianxiang Gao on April 13, 2015</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

				<p>In this blog, I am gonna cover the derivation of <strong>maximal marigin classifier</strong>. In order to understand the process, you need to know <em>KKT condition</em>, and <em>Lagrange dual</em>. If you are not so familier with those knowledge, please check another <a href="http://gaotx.com/blogs/2015/04/14/kkt-cvx/">blog</a>.</p>

<p><img src="/blogs/img/maximal-margin/maximal-margin-classifier.png" /></p>

<p>In the <a href="http://gaotx.com/blogs/2015/04/12/maximal-margin-classifier/">previous blog</a>, we derived the formulation of $M$, which is half of margin.</p>

<p>\begin{equation}
	M =\cfrac{y(\beta^Tx+\beta_0)}{||\beta||}
	\label{hypeplane}
\end{equation}</p>

<p>We want to find maximal margin hyperplane that has the farthest distances from observations. This separating hyperplane is just the solution to the optimization problem.</p>

<p>\begin{align} 
&amp;\underset{\beta,\beta_0, ||\beta||=1}{\text{max}}&amp; &amp; M\\ 
&amp; \text{subject to}
&amp; &amp; y_i(\beta^Tx_i+\beta_0) \geq M, \; i = 1, \ldots, n.
\end{align}</p>

<p>The set of constraints ensure that every observations are at least a signed distance $M$ from the hyperplane which is defined as $\beta^Tx+\beta_0=0$. Note that 
$
	||\beta||=1
$
is not realy constraint, since if $\beta^Tx+\beta_0=0$ defines a separating hyperplane, so does $k(\beta^Tx+\beta_0)=0$ for any $k\neq0$. However, it adds meaning to constraint (3), because of $\beta$ has been normalized, constraint (3) is the perpendicular distance from the <em>i</em>th observation to the hyperplane. On the other hand we can get rid of it by replacing the constrain (3) with</p>

<script type="math/tex; mode=display">\cfrac{1}{||\beta||}y_i(\beta^Tx_i+\beta_0) \geq M</script>

<p>(which redefines $\beta_0$) or equivalently</p>

<script type="math/tex; mode=display">y_i(\beta^Tx_i+\beta_0) \geq M||\beta||</script>

<table>
  <tbody>
    <tr>
      <td>Since for any $\beta_0$ and $\beta$ satisfying these inequaties, any positively scaled multiple satisfies them too, just like define a separating hyperplane, we can arbitrarily set $</td>
      <td> </td>
      <td>\beta</td>
      <td> </td>
      <td>= 1/M$. Thus (2)-(3) is equivalent to</td>
    </tr>
  </tbody>
</table>

<p>\begin{align} 
&amp;\underset{\beta,\beta_0}{\text{max}}&amp; &amp; \cfrac{1}{||\beta||}\\ 
&amp; \text{subject to}
&amp; &amp; y_i(\beta^Tx_i+\beta_0) \geq 1, \; i = 1, \ldots, n.
\end{align}</p>

<p>Maximizing $1/||\beta||$ is equivalent to minimizing $||\beta||$, and further minimizing
 $
 \cfrac{1}{2}||\beta||^2= \cfrac{1}{2}\beta^T\beta
 $
 for mathmatic convinence. Thus, (4)-(5) is equivalent to</p>

<p>\begin{align} 
&amp;\underset{\beta,\beta_0}{\text{min}}&amp; &amp; \cfrac{1}{2}\beta^T\beta \newline 
\label{primal:contraint}
&amp; \text{subject to}
&amp; &amp; y_i(\beta^Tx_i+\beta_0) \geq 1, \; i = 1, \ldots, n.
\end{align}</p>

<p>This is a convex optimization problem (quadratic criterion with linear inequality constraints). The Lagrange (primal) function is</p>

<p>\begin{equation}
  L(\beta,\beta_0,\alpha)=\cfrac{1}{2}\beta^T\beta-\sum_{i=1}^{n}\alpha_i[y_i(\beta^Tx_i+\beta_0)-1]
  \label{primal}
\end{equation}</p>

<p>and the corresponding dual function is</p>

<p>\begin{equation}
	g(\alpha) = \underset{\alpha}{\text{min}}\;L(\beta,\beta_0,\alpha)
	\label{dual:function1}
\end{equation}</p>

<p>In order to calculate the minimum of $L(\beta,\beta_0,\alpha)$, as it is quadratic function, we can set the partial derivatives for $\beta$ and $\beta_0$ to zero, we obtain:</p>

<p>\begin{equation}
	\beta = \sum_{i=1}^{n}\alpha_iy_ix_i
	\label{beta}
\end{equation}</p>

<p>\begin{equation}
	0 = \sum_{i=1}^{n}\alpha_iy_i
	\label{0}
\end{equation}</p>

<p>and substituing these in \eqref{primal} and \eqref{dual:function1}, we obtain the dual function</p>

<p>\begin{equation}
	g(\alpha) = \sum_{i=1}^{n}\alpha_1-\cfrac{1}{2}\sum_{i=1}^{n}\sum_{k=1}^{n}\alpha_i\alpha_ky_iy_kx_i^Tx_k
	\label{dual:function2}
\end{equation}</p>

<p>The dual problem is 
 \begin{align} 
&amp;\underset{\alpha}{\text{max}}&amp; &amp; g(\alpha) = \sum_{i=1}^{n}\alpha_1-\cfrac{1}{2}\sum_{i=1}^{n}\sum_{k=1}^{n}\alpha_i\alpha_ky_iy_kx_i^Tx_k\newline
\label{alpha}
&amp; \text{subject to}
&amp; &amp; \alpha_i \geq 0, \; i = 1, \ldots, n.\newline
&amp; &amp; &amp; \sum_{i=1}^{n}\alpha_iy_i = 0
\end{align}</p>

<p>The solution is obtained by solving this optimal problem through a standard software. In addition, the solution  satisfy the Karush-Kuhn-Tucher conditions, which include \eqref{primal:contraint},\eqref{beta}, \eqref{0}, \eqref{alpha} and</p>

<p>\begin{equation}
	\alpha_i[y_i(\beta^Tx_i+\beta_0)-1]=0, \forall i.
	\label{16}
\end{equation}</p>

<p>From these we can see that,</p>

<ul>
  <li>If $\alpha&gt;0$, then $y_i(\beta^Tx_i+\beta_0)=1$, in other words, $x_i$ is support vector;</li>
  <li>If $\alpha=0$, then $y_i(\beta^Tx_i+\beta_0)&gt;1$, and $x_i$ is not support vector.</li>
</ul>

<p>Solving the optimal problem obtains the values of $\alpha$. $\beta$ and $\beta_0$ can be obtained by \eqref{beta} and \eqref{16} for any suppor vectors.</p>

<p>The optimal separating hyperplane produces a function 
$
\hat{f}(x)=\hat{\beta}^Tx+\hat{\beta}_0
$
for classifying new test observations: 
\begin{equation}
	\hat{G} = sign\; \hat{f}(x)
	\label{17}
\end{equation}</p>

<p>Although none of the training observations fall in the margin, this will not necessarily the case for test observations. The intuition is that a large margin on the training data will lead to good separation on the test data. The higher the magnitude of $\hat{f}(x)$, the more confidence we are to classify the test observations.</p>


                <hr>

                <div id="disqus_thread"></div>
                    <script type="text/javascript">
                        /* * * CONFIGURATION VARIABLES * * */
                        var disqus_shortname = 'gaotx';
                        
                        /* * * DON'T EDIT BELOW THIS LINE * * */
                        (function() {
                            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                        })();   
                    </script>
                    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/blogs/2015/04/12/hypeplane-margin/" data-toggle="tooltip" data-placement="top" title="Suport vector machine #1 - Hyperplane and margin">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/blogs/2015/04/14/kkt-cvx/" data-toggle="tooltip" data-placement="top" title="Basic Concept of Convex Optimization">Next Post &rarr;</a>
                    </li>
                    
                </ul>

            </div>
        </div>
    </div>
</article>

<hr>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <a href="mailto:gaotx88@gmail.com"><i class="svg-icon email"></i></a>
                <a href="http://facebook.com/gaotx"><i class="svg-icon facebook"></i></a>
                <a href="http://github.com/gaotx"><i class="svg-icon github"></i></a>
                <a href="http://linkedin.com/in/gaotx"><i class="svg-icon linkedin"></i></a>
                <a href="https://twitter.com/gaotx88"><i class="svg-icon twitter"></i></a>
                <div class="footertext">
                  <p class="copyright">&copy; 2014-<script>document.write((new Date()).getFullYear().toString()); </script> Tianxiang Gao | Hosted by <a href="https://github.com">GitHub</a> &amp; Powered by <a href="http://jekyllrb.com/">Jekyll.</a></p>
                </div>
            </div>
        </div>
    </div>
</footer>
    <!-- jQuery -->
<script src="/blogs/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/blogs/js/bootstrap.min.js "></script>

<script src="/blogs/js/jquery.easing.min.js "></script>
<script src="/blogs/js/jquery.scrollTo.js "></script>
<script src="/blogs/js/wow.min.js "></script>
<!-- Custom Theme JavaScript -->
<script src="/blogs/js/custom.js "></script>

<!-- Equation numberling -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!-- MathJax.js -->
<script type="text/javascript"
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!--- Inline equation-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$']]}
});
</script>
<script type="text/javascript" src="path-to-mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


    

</body>

</html>
